{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42f82724",
   "metadata": {},
   "source": [
    "# Business Description Clustering with FinBERT\n",
    "\n",
    "This notebook implements a pipeline for clustering companies based on their business descriptions using FinBERT embeddings and cosine similarity.\n",
    "\n",
    "## Pipeline Steps\n",
    "1. Load and preprocess business descriptions\n",
    "2. Generate embeddings using FinBERT\n",
    "3. Filter relevant business function sentences\n",
    "4. Compute similarity scores\n",
    "5. Save filtered results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b5aa30",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Import required libraries and ensure dependencies are installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77c41af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import nltk\n",
    "import spacy\n",
    "import re\n",
    "import os\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download(\"punkt\", quiet=True)\n",
    "\n",
    "# Constants\n",
    "MODEL_NAME = \"yiyanghkust/finbert-tone\"\n",
    "SIMILARITY_THRESHOLD = 0.75\n",
    "DATA_PATH = \"../data/raw/management_support.csv\"\n",
    "RESULTS_PATH = \"../data/results/filtered_companies.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad9d496",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing\n",
    "Note: FinBERT is case-sensitive as it was trained on financial documents where capitalization carries meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77183fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Remove special characters and extra whitespace while preserving case.\n",
    "    \n",
    "    Args:\n",
    "        text: Input text to clean\n",
    "        \n",
    "    Returns:\n",
    "        Cleaned text with only letters and single spaces\n",
    "    \"\"\"\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "# Load and process dataset\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "df.rename(columns={\n",
    "    'Company Name': 'company_name', \n",
    "    'Business Description': 'business_description'\n",
    "}, inplace=True)\n",
    "\n",
    "# Clean descriptions and split into sentences\n",
    "df[\"cleaned_description\"] = df[\"business_description\"].apply(clean_text)\n",
    "df[\"sentences\"] = df[\"cleaned_description\"].apply(sent_tokenize)\n",
    "\n",
    "# Create one row per sentence\n",
    "df_expanded = df.explode(\"sentences\").rename(\n",
    "    columns={\"sentences\": \"individual_sentences\"}\n",
    ").reset_index(drop=True)\n",
    "\n",
    "print(\"Processed\", len(df), \"companies with\", len(df_expanded), \"total sentences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c923de",
   "metadata": {},
   "source": [
    "## Generate FinBERT Embeddings\n",
    "Create sentence embeddings using the FinBERT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b379111",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sentences(sentences: list) -> torch.Tensor:\n",
    "    \"\"\"Generate FinBERT embeddings for a list of sentences.\n",
    "    \n",
    "    Args:\n",
    "        sentences: List of sentences to encode\n",
    "        \n",
    "    Returns:\n",
    "        Tensor of sentence embeddings\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state[:, 0, :]\n",
    "\n",
    "# Initialize model\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Generate embeddings\n",
    "bd_sentences = df_expanded[\"individual_sentences\"].tolist()\n",
    "bd_embeddings = encode_sentences(bd_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6282091",
   "metadata": {},
   "source": [
    "## Filter Business Function Sentences\n",
    "Extract sentences that describe core business functions and activities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710424b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "FUNCTION_KEYWORDS = {\n",
    "    \"provides\", \"offers\", \"delivers\", \"specializes\", \"develops\",\n",
    "    \"manufactures\", \"produces\", \"designs\", \"implements\", \"supports\"\n",
    "}\n",
    "\n",
    "def describes_function(sentence: str) -> bool:\n",
    "    \"\"\"Check if sentence contains business function keywords.\n",
    "    \n",
    "    Args:\n",
    "        sentence: Input sentence to check\n",
    "        \n",
    "    Returns:\n",
    "        True if sentence contains function keywords\n",
    "    \"\"\"\n",
    "    return any(word in sentence.lower() for word in FUNCTION_KEYWORDS)\n",
    "\n",
    "def contains_business_activity(sentence: str) -> bool:\n",
    "    \"\"\"Check if sentence contains business-related verbs or nouns.\n",
    "    \n",
    "    Args:\n",
    "        sentence: Input sentence to analyze\n",
    "        \n",
    "    Returns:\n",
    "        True if sentence contains business activity indicators\n",
    "    \"\"\"\n",
    "    doc = nlp(sentence)\n",
    "    return any(token.pos_ in {\"VERB\", \"NOUN\"} and \n",
    "              token.dep_ in {\"ROOT\", \"pobj\"} for token in doc)\n",
    "\n",
    "# Filter sentences describing business functions\n",
    "df_filtered = df_expanded[df_expanded[\"individual_sentences\"].apply(\n",
    "    lambda x: describes_function(x) or contains_business_activity(x)\n",
    ")]\n",
    "\n",
    "print(f\"Filtered from {len(df_expanded)} to {len(df_filtered)} relevant sentences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf52c2fa",
   "metadata": {},
   "source": [
    "## Compute Similarity Scores\n",
    "Calculate cosine similarity between sentences and apply threshold filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2d49bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_companies(query: str, threshold: float = SIMILARITY_THRESHOLD) -> pd.DataFrame:\n",
    "    \"\"\"Find companies with similar business descriptions.\n",
    "    \n",
    "    Args:\n",
    "        query: Search query sentence\n",
    "        threshold: Minimum similarity score (default: 0.75)\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame of matching companies with similarity scores\n",
    "    \"\"\"\n",
    "    query_embedding = encode_sentences([query])\n",
    "    cosine_scores = F.cosine_similarity(query_embedding, bd_embeddings)\n",
    "    \n",
    "    # Filter results above threshold\n",
    "    filtered_results = [\n",
    "        {\n",
    "            \"company_name\": df_filtered.iloc[idx][\"company_name\"],\n",
    "            \"business_description\": df_filtered.iloc[idx][\"business_description\"],\n",
    "            \"similar_sentence\": df_filtered.iloc[idx][\"individual_sentences\"],\n",
    "            \"similarity_score\": round(score.item(), 4)\n",
    "        }\n",
    "        for score, idx in zip(cosine_scores, range(len(cosine_scores)))\n",
    "        if score.item() >= threshold\n",
    "    ]\n",
    "    \n",
    "    return pd.DataFrame(filtered_results)\n",
    "\n",
    "# Example search\n",
    "query = \"Provides consultancy services.\"\n",
    "results_df = find_similar_companies(query)\n",
    "print(f\"Found {len(results_df)} similar companies\")\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbcc305d",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078e6821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure results directory exists\n",
    "os.makedirs(os.path.dirname(RESULTS_PATH), exist_ok=True)\n",
    "\n",
    "# Save filtered results\n",
    "results_df.to_csv(RESULTS_PATH, index=False)\n",
    "print(f\"Results saved to {RESULTS_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
