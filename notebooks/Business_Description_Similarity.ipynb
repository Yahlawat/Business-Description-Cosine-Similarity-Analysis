{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42f82724",
   "metadata": {},
   "source": [
    "# Functional Company Search with SBERT and Cosine Similarity\n",
    "\n",
    "This project implements a natural language processing pipeline for **retrieving the most functionally similar companies** from a set of business descriptions using **SBERT embeddings** and **cosine similarity analysis**.\n",
    "\n",
    "The goal is to input a **functional prompt** (e.g., \"provides healthcare consulting\" or \"manufactures car parts\") and dynamically retrieve companies whose business descriptions best match the functional requirement. The model captures the semantic meaning of both the descriptions and the prompt to perform an accurate search.\n",
    "\n",
    "<img src=../Images/image.png alt=\"\" width=\"300\">\n",
    "\n",
    "**Objectives:**\n",
    "\n",
    "- Extract and preprocess company business descriptions\n",
    "- Generate high-quality financial text embeddings using SBERT\n",
    "- Encode user functional prompts into the same embedding space\n",
    "- Calculate cosine similarity scores between prompts and company descriptions\n",
    "- Retrieve and rank the top-matching companies based on semantic similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b5aa30",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "Import libraries, configure settings, and prepare the environment for data retrieval and processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d77c41af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ahlaw\\AppData\\Local\\Temp\\ipykernel_25976\\3492140873.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n",
      "c:\\Users\\ahlaw\\OneDrive - UBC\\Documents\\vscode\\Projects\\Business-Description-Similarity\\.venv\\Lib\\site-packages\\transformers\\utils\\generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import nltk\n",
    "import spacy\n",
    "import re\n",
    "import os\n",
    "import yfinance as yf\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download(\"punkt\", quiet=True)\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42) \n",
    "\n",
    "#Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4426d585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base directory for data\n",
    "base_dir = os.path.dirname(os.path.abspath(\".\"))\n",
    "\n",
    "# Define subdirectories\n",
    "data_dir = os.path.join(base_dir, \"Data\")\n",
    "embeddings_dir = os.path.join(data_dir, \"Embeddings\")\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "os.makedirs(embeddings_dir, exist_ok=True)\n",
    "\n",
    "# Define paths to save outputs\n",
    "company_bd_data_path = os.path.join(data_dir, \"company_bd_data.csv\")\n",
    "similarity_results_path = os.path.join(data_dir, \"similarity_results.csv\")\n",
    "embedding_save_path = os.path.join(embeddings_dir, \"bd_embeddings.pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6253873",
   "metadata": {},
   "source": [
    "## 2. Data Collection\n",
    "\n",
    "I use two main sources for gathering data on bsuiness descriptions for public companies:\n",
    "\n",
    "1. **Wikipedia**: To extract the current list of Russell 1000 companies, their ticker symbols, and industry classifications\n",
    "2. **Yahoo Finance API (via yfinance)**: To obtain detailed business descriptions for each company\n",
    "\n",
    "After retrieval, I perform basic data cleaning on the ticker symbols to ensure compatibility with the Yahoo Finance API format (replacing periods with dashes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fb83601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of errors: 578\n"
     ]
    }
   ],
   "source": [
    "# Extract Russell 1000 tickers from Wikipedia\n",
    "url = \"https://en.wikipedia.org/wiki/Russell_1000_Index\"\n",
    "russell1000_table = pd.read_html(url)[3] \n",
    "tickers = russell1000_table['Symbol'].tolist()\n",
    "\n",
    "# Replace periods with dashes (e.g., BRK.B -> BRK-B) as yfinance requires this format\n",
    "tickers = [ticker.replace('.', '-') for ticker in tickers]\n",
    "\n",
    "# Use yfinance to extract business descriptions for each ticker\n",
    "bd_data = []\n",
    "errors = []\n",
    "for ticker in tickers:\n",
    "    try:\n",
    "        stock = yf.Ticker(ticker)\n",
    "        info = stock.info\n",
    "        description = info.get('longBusinessSummary', '') \n",
    "        bd_data.append({'Symbol': ticker, 'long_bd': description})\n",
    "    except Exception as e:\n",
    "        errors.append({'Symbol': ticker, 'error': str(e)})\n",
    "\n",
    "# Add business description data to S&P 500 table\n",
    "df = pd.merge(russell1000_table, pd.DataFrame(bd_data), on='Symbol', how='left')\n",
    "\n",
    "# Print number of errors\n",
    "print(f\"Number of errors: {len(errors)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85dbfc91",
   "metadata": {},
   "source": [
    "Note: The Yahoo Finance API has rate limits that may cause errors when retrieving company data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c9a2663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 424 entries, 0 to 427\n",
      "Data columns (total 5 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   company            424 non-null    object\n",
      " 1   symbol             424 non-null    object\n",
      " 2   gics_sector        424 non-null    object\n",
      " 3   gics_sub_industry  342 non-null    object\n",
      " 4   long_bd            424 non-null    object\n",
      "dtypes: object(5)\n",
      "memory usage: 19.9+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Drop rows with missing business descriptions\n",
    "df = df[df['long_bd'].notna()]\n",
    "\n",
    "# Clean up column names for consistency\n",
    "df.columns = df.columns.str.lower()\n",
    "df.columns = df.columns.str.replace(\" \", \"_\")\n",
    "df.columns = df.columns.str.replace(\".\", \"_\")\n",
    "df.columns = df.columns.str.replace(\"-\", \"_\")\n",
    "\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa158939",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>symbol</th>\n",
       "      <th>gics_sector</th>\n",
       "      <th>gics_sub_industry</th>\n",
       "      <th>long_bd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10x Genomics</td>\n",
       "      <td>TXG</td>\n",
       "      <td>Health Care</td>\n",
       "      <td>Health Care Technology</td>\n",
       "      <td>10x Genomics, Inc., a life science technology ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3M</td>\n",
       "      <td>MMM</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>Industrial Conglomerates</td>\n",
       "      <td>3M Company provides diversified technology ser...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A. O. Smith</td>\n",
       "      <td>AOS</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>Building Products</td>\n",
       "      <td>A. O. Smith Corporation manufactures and marke...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAON</td>\n",
       "      <td>AAON</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>Building Products</td>\n",
       "      <td>AAON, Inc., together with its subsidiaries, en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Abbott Laboratories</td>\n",
       "      <td>ABT</td>\n",
       "      <td>Health Care</td>\n",
       "      <td>Health Care Equipment</td>\n",
       "      <td>Abbott Laboratories, together with its subsidi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               company symbol  gics_sector         gics_sub_industry  \\\n",
       "0         10x Genomics    TXG  Health Care    Health Care Technology   \n",
       "1                   3M    MMM  Industrials  Industrial Conglomerates   \n",
       "2          A. O. Smith    AOS  Industrials         Building Products   \n",
       "3                 AAON   AAON  Industrials         Building Products   \n",
       "4  Abbott Laboratories    ABT  Health Care     Health Care Equipment   \n",
       "\n",
       "                                             long_bd  \n",
       "0  10x Genomics, Inc., a life science technology ...  \n",
       "1  3M Company provides diversified technology ser...  \n",
       "2  A. O. Smith Corporation manufactures and marke...  \n",
       "3  AAON, Inc., together with its subsidiaries, en...  \n",
       "4  Abbott Laboratories, together with its subsidi...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display top rows \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a92e3aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save comapny busienss description data\n",
    "df.to_csv(company_bd_data_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad9d496",
   "metadata": {},
   "source": [
    "## 3. Text Cleaning and Functional Filtering\n",
    "\n",
    "I preprocess business descriptions to focus only on sentences that describe functional operations, removing generic or non-business information:\n",
    "\n",
    "- Clean text by removing extra whitespace.\n",
    "- Split descriptions into individual sentences.\n",
    "- Expand the dataset to one row per sentence.\n",
    "- Filter out sentences about locations, history, or organizational details.\n",
    "- Keep only sentences that capture actual business activities.<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c37a7e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 424 companies into 2800 total sentences.\n"
     ]
    }
   ],
   "source": [
    "# Clean text function\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Remove extra whitespace while preserving case.\"\"\"\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "# Copy original dataframe and preprocess text\n",
    "df_cleaned = df.copy()\n",
    "df_cleaned[\"clean_long_bd\"] = df_cleaned[\"long_bd\"].apply(clean_text)\n",
    "df_cleaned[\"sentences\"] = df_cleaned[\"clean_long_bd\"].apply(sent_tokenize)\n",
    "\n",
    "# Expand each description into individual sentences\n",
    "df_expanded = df_cleaned.explode(\"sentences\").rename(\n",
    "    columns={\"sentences\": \"individual_sentence\"}\n",
    ").reset_index(drop=True)\n",
    "\n",
    "print(f\"Processed {len(df_cleaned)} companies into {len(df_expanded)} total sentences.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a98a051b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Spacy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# List of phrases indicating non-functional sentences\n",
    "NON_FUNCTION_KEYWORDS = {\n",
    "    \"headquartered in\", \"located in\", \"offices in\", \"based in\", \"established in\",\n",
    "    \"founded in\", \"incorporated in\", \"previously known as\", \"formerly known as\",\n",
    "    \"renamed to\", \"was founded\"\n",
    "}\n",
    "\n",
    "def describes_non_function(sentence: str) -> bool:\n",
    "    \"\"\"Check if a sentence contains non-functional keywords (e.g., locations, history).\"\"\"\n",
    "    return any(phrase in sentence.lower() for phrase in NON_FUNCTION_KEYWORDS)\n",
    "\n",
    "def filter_functional_sentences(df: pd.DataFrame) -> (pd.DataFrame, pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Remove non-functional sentences from the expanded dataframe.\n",
    "    \"\"\"\n",
    "    mask = df[\"individual_sentence\"].apply(\n",
    "        lambda x: not describes_non_function(x)\n",
    "    )\n",
    "    df_filtered = df[mask].reset_index(drop=True)\n",
    "    df_removed = df[~mask].reset_index(drop=True)\n",
    "    return df_filtered, df_removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3432378a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of non-functional sentences removed: 536\n"
     ]
    }
   ],
   "source": [
    "# Apply the filtering\n",
    "df_filtered, df_removed = filter_functional_sentences(df_expanded)\n",
    "\n",
    "# Print stats\n",
    "print(f\"Number of non-functional sentences removed: {len(df_removed)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "481bc524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Examples of removed non-functional sentences:\n",
      "0    The company was formerly known as 10X Technolo...\n",
      "1    10x Genomics, Inc. was incorporated in 2012 an...\n",
      "2    3M Company was founded in 1902 and is headquar...\n",
      "3    A. O. Smith Corporation was founded in 1874 an...\n",
      "4    AAON, Inc. was incorporated in 1987 and is hea...\n",
      "5    Abbott Laboratories was founded in 1888 and is...\n",
      "6    The company was incorporated in 2012 and is he...\n",
      "7    Acadia Healthcare Company, Inc. was founded in...\n",
      "8    The company was founded in 1951 and is based i...\n",
      "9    Acuity Inc. was formerly known as Acuity Brand...\n",
      "Name: individual_sentence, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Print the some removed sentences\n",
    "print(\"\\nExamples of removed non-functional sentences:\")\n",
    "print(df_removed[\"individual_sentence\"][0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c923de",
   "metadata": {},
   "source": [
    "## 4. Sentence Embedding Generation\n",
    "\n",
    "In this step, I generate high-quality sentence embeddings for the cleaned business descriptions by:\n",
    "\n",
    "- Loading the **multi-qa-mpnet-base-cos-v1** model, a state-of-the-art Sentence-BERT (SBERT) model.\n",
    "- This model is specifically optimized for semantic search and question-answer retrieval tasks, making it ideal for capturing the functional meaning behind business descriptions.\n",
    "- Encoding each functional sentence into a dense vector representation in a shared semantic space.\n",
    "- Using batch processing for efficient handling of large datasets.\n",
    "- Saving the generated embeddings for faster reuse in future steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2439573f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "304ef7b393cb4c83a94cb1def6243345",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e14f05d765eb4afc841348894e00c7c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35a0bd2cc2094b3da6e9e0df3607824b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4d46098aa4748929dc2835c4c4ec450",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab3759619e1a45d1a4e71506e5621245",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65e9799445d14310a590b6ddbc30a348",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings for 2264 sentences.\n"
     ]
    }
   ],
   "source": [
    "# SBERT Model\n",
    "SBERT_MODEL = \"sentence-transformers/multi-qa-mpnet-base-cos-v1\"\n",
    "BATCH_SIZE = 64  # Batch size for encoding\n",
    "\n",
    "# Load FinBERT model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(SBERT_MODEL)\n",
    "model = AutoModel.from_pretrained(SBERT_MODEL)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "def encode_sentences(sentences: list, batch_size: int = BATCH_SIZE) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Generate SBERT embeddings for a list of sentences, using batching for efficiency.\n",
    "    \n",
    "    Args:\n",
    "        sentences: List of input sentences.\n",
    "        batch_size: Number of sentences to encode at once.\n",
    "    \n",
    "    Returns:\n",
    "        Tensor of embeddings corresponding to each sentence.\n",
    "    \"\"\"\n",
    "    embeddings = []\n",
    "    \n",
    "    for i in range(0, len(sentences), batch_size):\n",
    "        batch = sentences[i:i + batch_size]\n",
    "        inputs = tokenizer(\n",
    "            batch,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            batch_embeddings = outputs.last_hidden_state[:, 0, :]  # CLS token\n",
    "            embeddings.append(batch_embeddings.cpu())  # Move to CPU to save GPU memory\n",
    "    \n",
    "    return torch.cat(embeddings, dim=0)\n",
    "\n",
    "# Prepare sentences for embedding\n",
    "bd_sentences = df_filtered[\"individual_sentence\"].tolist()\n",
    "\n",
    "# Generate embeddings\n",
    "bd_embeddings = encode_sentences(bd_sentences)\n",
    "\n",
    "print(f\"Generated embeddings for {len(bd_sentences)} sentences.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "23b3adcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings saved.\n"
     ]
    }
   ],
   "source": [
    "# Save embeddings\n",
    "torch.save(bd_embeddings, embedding_save_path)\n",
    "print(f\"Embeddings saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf52c2fa",
   "metadata": {},
   "source": [
    "## 5. Prompt-Based Similarity Search\n",
    "\n",
    "In this step, I perform a semantic search to retrieve the companies most aligned with a functional prompt (e.g., \"provides healthcare consulting\") by:\n",
    "\n",
    "- Encoding the functional prompt into a dense vector using SBERT embeddings.\n",
    "- Computing cosine similarity between the prompt and all business description sentence embeddings.\n",
    "- Grouping matched sentences by company and calculating the average similarity score per company.\n",
    "- Sorting and returning the Top-N companies with the highest functional similarity to the prompt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4c8de282",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_top_n_similar_companies_grouped(\n",
    "    query: str,\n",
    "    bd_embeddings: torch.Tensor,\n",
    "    df_filtered: pd.DataFrame,\n",
    "    top_n: int = 5\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Find Top-N companies functionally similar to a given query,\n",
    "    grouped by company with lists of matched sentences and similarity scores.\n",
    "\n",
    "    Args:\n",
    "        query: Functional prompt (e.g., \"Manufactures automobile parts.\")\n",
    "        bd_embeddings: Precomputed embeddings of business description sentences.\n",
    "        df_filtered: DataFrame containing company names, descriptions, and individual sentences.\n",
    "        top_n: Number of top companies to return (default: 5).\n",
    "\n",
    "    Returns:\n",
    "        DataFrame where each row represents a company with:\n",
    "        - Company name\n",
    "        - Business description\n",
    "        - List of similar sentences\n",
    "        - List of similarity scores\n",
    "        - Average similarity score\n",
    "    \"\"\"\n",
    "    # Encode the query\n",
    "    query_embedding = encode_sentences([query])\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    cosine_scores = F.cosine_similarity(query_embedding, bd_embeddings)\n",
    "\n",
    "    # Collect all matches without applying a threshold\n",
    "    match_records = [\n",
    "        {\n",
    "            \"company_name\": df_filtered.iloc[idx][\"company\"],\n",
    "            \"business_description\": df_filtered.iloc[idx][\"long_bd\"],\n",
    "            \"similar_sentence\": df_filtered.iloc[idx][\"individual_sentence\"],\n",
    "            \"similarity_score\": round(score.item(), 4)\n",
    "        }\n",
    "        for idx, score in enumerate(cosine_scores)\n",
    "    ]\n",
    "\n",
    "    # Create a temporary DataFrame\n",
    "    temp_df = pd.DataFrame(match_records)\n",
    "\n",
    "    # Group by company and aggregate sentences and scores into lists\n",
    "    grouped = temp_df.groupby([\"company_name\", \"business_description\"]).agg({\n",
    "        \"similar_sentence\": list,\n",
    "        \"similarity_score\": list\n",
    "    }).reset_index()\n",
    "\n",
    "    # Calculate average similarity per company\n",
    "    grouped[\"average_similarity\"] = grouped[\"similarity_score\"].apply(lambda scores: sum(scores) / len(scores))\n",
    "\n",
    "    # Sort by average similarity and take top_n\n",
    "    top_companies = grouped.sort_values(by=\"average_similarity\", ascending=False).head(top_n).reset_index(drop=True)\n",
    "\n",
    "    return top_companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "fc6859d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 companies matching the query:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company_name</th>\n",
       "      <th>business_description</th>\n",
       "      <th>similar_sentence</th>\n",
       "      <th>similarity_score</th>\n",
       "      <th>average_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CrowdStrike</td>\n",
       "      <td>CrowdStrike Holdings, Inc. provides cybersecur...</td>\n",
       "      <td>[CrowdStrike Holdings, Inc. provides cybersecu...</td>\n",
       "      <td>[0.4261, 0.3993, 0.6595, 0.5276]</td>\n",
       "      <td>0.5031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CDW</td>\n",
       "      <td>CDW Corporation provides information technolog...</td>\n",
       "      <td>[CDW Corporation provides information technolo...</td>\n",
       "      <td>[0.4202, 0.4142, 0.5439, 0.4883, 0.5479, 0.5966]</td>\n",
       "      <td>0.5019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Amdocs</td>\n",
       "      <td>Amdocs Limited, through its subsidiaries, prov...</td>\n",
       "      <td>[Amdocs Limited, through its subsidiaries, pro...</td>\n",
       "      <td>[0.2483, 0.5449, 0.4429, 0.638, 0.5773]</td>\n",
       "      <td>0.4903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cognizant</td>\n",
       "      <td>Cognizant Technology Solutions Corporation, a ...</td>\n",
       "      <td>[Cognizant Technology Solutions Corporation, a...</td>\n",
       "      <td>[0.3989, 0.4208, 0.6492, 0.4784, 0.4188, 0.5332]</td>\n",
       "      <td>0.4832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fidelity National Information Services</td>\n",
       "      <td>Fidelity National Information Services, Inc. e...</td>\n",
       "      <td>[Fidelity National Information Services, Inc. ...</td>\n",
       "      <td>[0.2981, 0.4926, 0.5754, 0.5283]</td>\n",
       "      <td>0.4736</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             company_name  \\\n",
       "0                             CrowdStrike   \n",
       "1                                     CDW   \n",
       "2                                  Amdocs   \n",
       "3                               Cognizant   \n",
       "4  Fidelity National Information Services   \n",
       "\n",
       "                                business_description  \\\n",
       "0  CrowdStrike Holdings, Inc. provides cybersecur...   \n",
       "1  CDW Corporation provides information technolog...   \n",
       "2  Amdocs Limited, through its subsidiaries, prov...   \n",
       "3  Cognizant Technology Solutions Corporation, a ...   \n",
       "4  Fidelity National Information Services, Inc. e...   \n",
       "\n",
       "                                    similar_sentence  \\\n",
       "0  [CrowdStrike Holdings, Inc. provides cybersecu...   \n",
       "1  [CDW Corporation provides information technolo...   \n",
       "2  [Amdocs Limited, through its subsidiaries, pro...   \n",
       "3  [Cognizant Technology Solutions Corporation, a...   \n",
       "4  [Fidelity National Information Services, Inc. ...   \n",
       "\n",
       "                                   similarity_score  average_similarity  \n",
       "0                  [0.4261, 0.3993, 0.6595, 0.5276]              0.5031  \n",
       "1  [0.4202, 0.4142, 0.5439, 0.4883, 0.5479, 0.5966]              0.5019  \n",
       "2           [0.2483, 0.5449, 0.4429, 0.638, 0.5773]              0.4903  \n",
       "3  [0.3989, 0.4208, 0.6492, 0.4784, 0.4188, 0.5332]              0.4832  \n",
       "4                  [0.2981, 0.4926, 0.5754, 0.5283]              0.4736  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Similarity seaarch - example useage\n",
    "query = \"Provides cybersecurity services.\"\n",
    "similarity_results_df = find_top_n_similar_companies_grouped(query, bd_embeddings, df_filtered, top_n=5)\n",
    "\n",
    "print(f\"Top {len(similarity_results_df)} companies matching the query:\")\n",
    "similarity_results_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "8d461701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "similarity_results_df.to_csv(similarity_results_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbcc305d",
   "metadata": {},
   "source": [
    "## 6. Conclusion\n",
    "\n",
    "This project successfully implemented a prompt-based semantic search pipeline to retrieve functionally similar companies based on business descriptions.  \n",
    "By leveraging SBERT embeddings and cosine similarity analysis, the project is able to:\n",
    "\n",
    "- Capture the functional meaning behind both company descriptions and user prompts.\n",
    "- Group and rank companies based on the strength of their semantic similarity.\n",
    "\n",
    "To further enhance the system, pre-filtering the business description dataset based on sector classifications, and financial metrics could:\n",
    "\n",
    "- Reduce computational overhead by reducing data.\n",
    "- Improve retrieval quality by focusing the search on more contextually relevant companies.\n",
    "- Enable more targeted functional comparisons in specialized domains.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
